{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eef5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.error import HTTPError\n",
    "import pathlib\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265d1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter:\n",
    "    \"\"\" \n",
    "    Given a year of interest, finds the relevant data files and randomly selects 100 coastal and 100 inland counties.\n",
    "    Outputs a csv of those sample counties for that year. \n",
    "    Each county has it's respective population, average property damage from a storm event, and if it is coastal.\n",
    "\n",
    "    Attributes:\n",
    "        year: int, year of interest\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, year, random_seed=None, zone=False, ignore_zero_damage=False):\n",
    "        self.year = year\n",
    "        self.coastal_sample = None\n",
    "        self.inland_sample = None\n",
    "        self.total_sample = None\n",
    "        self.random_seed = random_seed\n",
    "        self.zone = zone\n",
    "        self.ignore_zero_damage = ignore_zero_damage\n",
    "        \n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "\n",
    "        self.PARENT_DIRECTORY = pathlib.Path().resolve()\n",
    "\n",
    "        self.non_MAINLAND_STATES_FIPS = ['02','15'] + ['60','64','66','68','69','70','72','74','78']\n",
    "\n",
    "        self.COASTAL_COUNTIES_2024 = self.PARENT_DIRECTORY/ \"Required_CSV\" / \"Coastal_Counties_2024.csv\"\n",
    "        self.CENSUS_DIRECTORY_URL = self.get_population_totals_county_url(self.year)\n",
    "        self.STORM_DIRECTORY_URL = self.get_stormevents_detail_url(self.year)\n",
    "\n",
    "        self.zone_county_reference = self.PARENT_DIRECTORY / \"Required_CSV\" / \"zone_county_reference.csv\"\n",
    "\n",
    "        self.data_path = self.PARENT_DIRECTORY / f\"{self.year}\"\n",
    "\n",
    "        self.download_data()\n",
    "\n",
    "        # check that data was downloaded successfully\n",
    "        census_csv_path = self.data_path / \"census.csv\"\n",
    "        storm_csv_path = self.data_path / \"storm.csv\"\n",
    "\n",
    "        if not census_csv_path.exists():\n",
    "            raise ValueError(f\"Census CSV file not found in the project folder for year {self.year} after download.\")\n",
    "        \n",
    "        if not storm_csv_path.exists():\n",
    "            raise ValueError(f\"Storm CSV file not found in the project folder for year {self.year} after download.\")\n",
    "\n",
    "        self.data_paths = self.find_data()\n",
    "\n",
    "        self.clean_all_dfs()\n",
    "\n",
    "        # get counties with damage and store in self.damage_by_county for future use\n",
    "        self.damage_by_county = None\n",
    "        self.counties_with_damage = self.counties_with_damage_getter()\n",
    "        \n",
    "        self.make_final_dataset()\n",
    "\n",
    "        # delete census and storm csv to save space and if re-running the code it will re-download fresh copies.\n",
    "        try:\n",
    "            os.remove(census_csv_path)\n",
    "            os.remove(storm_csv_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "    \n",
    "    def get_population_totals_county_url(self,year: int) -> str:\n",
    "        \"\"\"\n",
    "        Return direct URL to the 'population totals by county' file (CSV or ZIP)\n",
    "        for the requested year from the Census Popest datasets.\n",
    "\n",
    "        Args:\n",
    "            year (int): year to find (e.g. 2024)\n",
    "\n",
    "        Returns:\n",
    "            str: direct URL to the matching file (CSV or ZIP)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if no matching file is found\n",
    "            requests.HTTPError: for HTTP errors while fetching pages\n",
    "        \"\"\"\n",
    "\n",
    "        BASE_URL = \"https://www2.census.gov/programs-surveys/popest/datasets/\"\n",
    "        # 1) Get top-level ranges\n",
    "        r = requests.get(BASE_URL)\n",
    "        r.raise_for_status()\n",
    "        ranges = re.findall(r'href=\"(\\d{4}-\\d{4})/', r.text)\n",
    "        if not ranges:\n",
    "            raise ValueError(\"No dataset ranges found on the Census site.\")\n",
    "\n",
    "        # 2) pick the range that contains the year\n",
    "        target_range = None\n",
    "        for rng in ranges:\n",
    "            start, end = map(int, rng.split('-'))\n",
    "            if start <= year <= end:\n",
    "                target_range = rng\n",
    "                break\n",
    "        if not target_range:\n",
    "            raise ValueError(f\"No dataset range covers year {year}.\")\n",
    "\n",
    "        range_url = urljoin(BASE_URL, f\"{target_range}/\")\n",
    "\n",
    "        # 3) find counties/ subfolder\n",
    "        r = requests.get(range_url)\n",
    "        r.raise_for_status()\n",
    "        subfolders = re.findall(r'href=\"([^\"]+/)\"', r.text)\n",
    "        counties_subfolder = next((s for s in subfolders if \"counties\" in s.lower()), None)\n",
    "        if not counties_subfolder:\n",
    "            raise ValueError(f\"No 'counties' folder found in range {target_range}.\")\n",
    "\n",
    "        counties_url = urljoin(range_url, counties_subfolder)\n",
    "\n",
    "        # 4) fetch counties page & collect:\n",
    "        #   - any files directly listed on counties page\n",
    "        #   - any first-level subfolders (e.g. totals/) and files inside them\n",
    "        r = requests.get(counties_url)\n",
    "        r.raise_for_status()\n",
    "        counties_html = r.text\n",
    "\n",
    "        # gather files on the counties page itself\n",
    "        files = re.findall(r'href=\"([^\"]+\\.(?:csv|zip))\"', counties_html, re.IGNORECASE)\n",
    "        file_urls = [urljoin(counties_url, f) for f in files]\n",
    "\n",
    "        # gather first-level subfolders and search them too\n",
    "        subfolders_level1 = re.findall(r'href=\"([^\"]+/)\"', counties_html)\n",
    "        # filter out parent/other non-useful folders\n",
    "        subfolders_level1 = [s for s in subfolders_level1 if not s.startswith('?') and s != 'Parent Directory/']\n",
    "\n",
    "        for sub in subfolders_level1:\n",
    "            sub_url = urljoin(counties_url, sub)\n",
    "            try:\n",
    "                rr = requests.get(sub_url)\n",
    "                rr.raise_for_status()\n",
    "                sub_files = re.findall(r'href=\"([^\"]+\\.(?:csv|zip))\"', rr.text, re.IGNORECASE)\n",
    "                file_urls.extend(urljoin(sub_url, f) for f in sub_files)\n",
    "            except requests.HTTPError:\n",
    "                # ignore inaccessible subfolders and continue\n",
    "                continue\n",
    "\n",
    "        # 5) pick the best matching file for the year\n",
    "        # prefer filenames that include 'co-est' and the year and 'alldata' or 'pop'\n",
    "        pattern = re.compile(rf'(co[-_]?est|coest|county).*{year}.*\\.(csv|zip)', re.IGNORECASE)\n",
    "        # fallback: any file with the year in its name\n",
    "        fallback_pattern = re.compile(rf'.*{year}.*\\.(csv|zip)', re.IGNORECASE)\n",
    "\n",
    "        for url in file_urls:\n",
    "            if pattern.search(url):\n",
    "                return url\n",
    "\n",
    "        for url in file_urls:\n",
    "            if fallback_pattern.search(url):\n",
    "                return url\n",
    "\n",
    "        raise ValueError(f\"No county population totals file found for year {year} in {counties_url}.\")\n",
    "\n",
    "    def get_stormevents_detail_url(self,year: int) -> str:\n",
    "        base_url = \"https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "        r = requests.get(base_url)\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        pattern = re.compile(\n",
    "            fr'StormEvents_details-ftp_v1\\.0_d{year}_c\\d+\\.csv\\.gz'\n",
    "        )\n",
    "        match = pattern.search(r.text)\n",
    "        if match:\n",
    "            return base_url + match.group(0)\n",
    "        else:\n",
    "            raise ValueError(f\"No StormEvents details file found for year {year}\")\n",
    "        \n",
    "    def download_data(self):\n",
    "        \"\"\" Download census and storm data for the given year. Unzip files as needed. \"\"\"\n",
    "\n",
    "        census_url = self.CENSUS_DIRECTORY_URL\n",
    "        storm_url = self.STORM_DIRECTORY_URL\n",
    "\n",
    "        # if files already exist, delete them to re-download fresh copies\n",
    "        census_csv_path = self.data_path / \"census.csv\"\n",
    "        storm_csv_path = self.data_path / \"storm.csv\"\n",
    "        if census_csv_path.exists():\n",
    "            os.remove(census_csv_path)\n",
    "        if storm_csv_path.exists():\n",
    "            os.remove(storm_csv_path)\n",
    "\n",
    "        # check if year folder exists, if not create it\n",
    "        if not self.data_path.exists():\n",
    "            self.data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Download and unzip census data\n",
    "        census_year_folder = f\"co-est{self.year}-alldata\"\n",
    "        census_zip_path = self.data_path / f\"{census_year_folder}.zip\"\n",
    "        urlretrieve(census_url, census_zip_path)\n",
    "        try:\n",
    "            with zipfile.ZipFile(census_zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.data_path / \"census\")\n",
    "        except zipfile.BadZipFile:\n",
    "            # If the file is not a zip file, move it to the census folder directly\n",
    "            os.rename(census_zip_path, self.data_path / \"census.csv\")\n",
    "        else:\n",
    "            os.remove(census_zip_path)\n",
    "\n",
    "        #check for census csv in project folder. if not found, raise error.\n",
    "        census_csv_path = self.data_path / \"census.csv\"\n",
    "        if not census_csv_path.exists():\n",
    "            raise ValueError(f\"Census CSV file not found after attempted download in the project folder for year {self.year}.\")\n",
    "\n",
    "        # Download and unzip gz file of storm data\n",
    "        storm_zip_path = self.data_path / f\"storm_{self.year}.csv.gz\"\n",
    "        try:\n",
    "            urlretrieve(storm_url, storm_zip_path)\n",
    "        except HTTPError:\n",
    "            raise ValueError(f\"Storm data for year {self.year} has been modified since creation of code.\\\n",
    "                              Contact creator.\")\n",
    "        \n",
    "        # Unzip gz file\n",
    "        with gzip.open(storm_zip_path, 'rb') as f_in:\n",
    "            with open(self.data_path / \"storm.csv\", 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(storm_zip_path)\n",
    "\n",
    "        # check for storm csv in project folder. if not found, raise error.\n",
    "        storm_csv_path = self.data_path / \"storm.csv\"\n",
    "        if not storm_csv_path.exists():\n",
    "            raise ValueError(f\"Storm CSV file not found after attempted download in the project folder for year {self.year}.\")\n",
    "\n",
    "    def find_data(self):\n",
    "        \"\"\" \n",
    "        Search for sub folder titled with self.year in the current folder. \n",
    "        Return path to each csv file in that folder as (pop, coastal, storm).\n",
    "        \"\"\"\n",
    "\n",
    "        census_path = self.data_path / \"census.csv\"\n",
    "        storm_path = self.data_path / \"storm.csv\"\n",
    "\n",
    "        return (census_path, self.COASTAL_COUNTIES_2024, storm_path)\n",
    "\n",
    "    def convert_damage(self, damage_str):\n",
    "        \"\"\" Convert damage string with K, M, B suffixes to numeric value.\"\"\"\n",
    "        if isinstance(damage_str, str):\n",
    "            damage_str = damage_str.upper()\n",
    "            if 'K' in damage_str:\n",
    "                return float(damage_str.replace('K', '')) * 1000\n",
    "            if 'M' in damage_str:\n",
    "                return float(damage_str.replace('M', '')) * 1000000\n",
    "            if 'B' in damage_str:\n",
    "                return float(damage_str.replace('B', '')) * 1000000000\n",
    "        return pd.to_numeric(damage_str, errors='coerce')\n",
    "\n",
    "    def clean_storm_df(self):\n",
    "        \"\"\" clean storm dataframe and overwrite csv. \"\"\"\n",
    "        storm_df = pd.read_csv(self.data_paths[2], encoding='latin1', dtype=str)\n",
    "\n",
    "        # clean fips codes to be strings with leading zeros as needed.\n",
    "        storm_df['STATE_FIPS'] = storm_df['STATE_FIPS'].astype(str).str.zfill(2)\n",
    "        storm_df['CZ_FIPS'] = storm_df['CZ_FIPS'].astype(str).str.zfill(3)\n",
    "        storm_df['FIPS'] = storm_df['STATE_FIPS'] + storm_df['CZ_FIPS']\n",
    "\n",
    "        # write storm df to csv for debugging\n",
    "        #storm_df.to_csv(self.data_paths[2].with_name(f\"storm_preclean_debug_{self.year}_{self.random_seed}.csv\"), index=False)\n",
    "\n",
    "        # remove non-mainland states\n",
    "        storm_df = storm_df[~storm_df['STATE_FIPS'].isin(self.non_MAINLAND_STATES_FIPS)]\n",
    "\n",
    "        if not self.zone:\n",
    "            #remove rows where CZ_TYPE is not 'C' (county)\n",
    "            storm_df = storm_df[storm_df['CZ_TYPE'] == 'C']\n",
    "\n",
    "        #remove all columns except STATE_FIPS, CZ_FIPS, FIPS, DAMAGE_PROPERTY\n",
    "        storm_df = storm_df[['STATE_FIPS', 'CZ_FIPS', 'FIPS', 'DAMAGE_PROPERTY']]\n",
    "\n",
    "        # remove rows with missing DAMAGE_PROPERTY\n",
    "        storm_df = storm_df[storm_df['DAMAGE_PROPERTY'].notna()]\n",
    "\n",
    "        # write storm df to csv pre conversion for debugging\n",
    "        #storm_df.to_csv(self.data_paths[2].with_name(f\"storm_preconversion_debug_{self.year}_{self.random_seed}.csv\"), index=False)\n",
    "        \n",
    "        # convert DAMAGE_PROPERTY to numeric, handling K, M, B suffixes\n",
    "        storm_df['DAMAGE_PROPERTY'] = storm_df['DAMAGE_PROPERTY'].apply(self.convert_damage)\n",
    "\n",
    "        # write storm df to csv for debugging\n",
    "        #storm_df.to_csv(self.data_paths[2].with_name(f\"storm_debug_{self.year}_{self.random_seed}.csv\"), index=False)\n",
    "\n",
    "        # overwrite storm csv\n",
    "        storm_df.to_csv(self.data_paths[2], index=False)\n",
    "\n",
    "        #close the dataframe\n",
    "        del storm_df\n",
    "\n",
    "    def clean_census_df(self):\n",
    "        \"\"\" clean census dataframe and overwrite csv. \"\"\"\n",
    "        census_df = pd.read_csv(self.data_paths[0], encoding='latin1', dtype=str)\n",
    "\n",
    "        # clean fips codes to be strings with leading zeros as needed.\n",
    "        census_df['STATE'] = census_df['STATE'].astype(str).str.zfill(2)\n",
    "        census_df['COUNTY'] = census_df['COUNTY'].astype(str).str.zfill(3)\n",
    "\n",
    "        # add full fips column\n",
    "        census_df['FIPS'] = census_df['STATE'] + census_df['COUNTY']\n",
    "\n",
    "        #remove non-mainland states\n",
    "        census_df = census_df[~census_df['STATE'].isin(self.non_MAINLAND_STATES_FIPS)]\n",
    "\n",
    "        # remove all columns except STATE, COUNTY, CTYNAME, STNAME, POPESTIMATE for years of interest\n",
    "        pop_col = f'POPESTIMATE{self.year}'\n",
    "        census_df = census_df[['STATE', 'COUNTY', 'CTYNAME', 'STNAME', pop_col, 'FIPS']]\n",
    "\n",
    "        # overwrite census csv\n",
    "        census_df.to_csv(self.data_paths[0], index=False)\n",
    "\n",
    "        #close the dataframe\n",
    "        del census_df\n",
    "\n",
    "    def clean_coastal_counties_df(self):\n",
    "        \"\"\" clean coastal counties dataframe and overwrite csv. \"\"\"\n",
    "        coastal_counties_df = pd.read_csv(self.data_paths[1], encoding='latin1', dtype=str)\n",
    "\n",
    "        # clean fips codes to be strings with leading zeros as needed.\n",
    "        coastal_counties_df['countyfips'] = coastal_counties_df['countyfips'].astype(str).str.zfill(5)\n",
    "        coastal_counties_df['statefips'] = coastal_counties_df['statefips'].astype(str).str.zfill(2)\n",
    "\n",
    "        # remove non-mainland states\n",
    "        coastal_counties_df = coastal_counties_df[~coastal_counties_df['statefips'].isin(self.non_MAINLAND_STATES_FIPS)]\n",
    "\n",
    "        # remove all columns except countyfips and statefips\n",
    "        coastal_counties_df = coastal_counties_df[['countyfips', 'statefips']]\n",
    "\n",
    "        # overwrite coastal counties csv\n",
    "        coastal_counties_df.to_csv(self.data_paths[1], index=False)\n",
    "\n",
    "        #close the dataframe\n",
    "        del coastal_counties_df\n",
    "\n",
    "    def clean_all_dfs(self):\n",
    "        \"\"\" do all cleaning processes for all dataframes and overwrite csvs. \"\"\"\n",
    "        self.clean_storm_df()\n",
    "        self.clean_census_df()\n",
    "        self.clean_coastal_counties_df()\n",
    "\n",
    "    def counties_with_damage_getter(self):\n",
    "        \"\"\" Return list of county fips where the TOTAL property damage from storms in that year is greater than zero.\n",
    "\n",
    "        Returns:\n",
    "            counties_with_damage: list of county fips with non-zero property damage\n",
    "            \n",
    "        \"\"\"\n",
    "        storm_df = pd.read_csv(self.data_paths[2], encoding='latin1', dtype=str)\n",
    "\n",
    "        # Group by FIPS making damage a list containing all damage values for that county fips\n",
    "        damage_by_county = storm_df.groupby('FIPS')['DAMAGE_PROPERTY'].apply(list)\n",
    "\n",
    "        # Sum the damage values for each county fips\n",
    "        damage_by_county = damage_by_county.apply(lambda x: sum([float(d) for d in x if pd.notnull(d)]))\n",
    "\n",
    "\n",
    "        #write damage by county to csv for debugging\n",
    "        #damage_by_county.to_csv(self.data_path / f\"damage_by_county_debug_{self.year}.csv\", header=['Total_Damage_Property'])\n",
    "\n",
    "\n",
    "        # while where at it, store damage_by_county in self for future use\n",
    "        self.damage_by_county = damage_by_county\n",
    "\n",
    "        # Filter counties with total damage > 0\n",
    "        counties_with_damage = damage_by_county[damage_by_county != 0.0].index.tolist()\n",
    "\n",
    "\n",
    "        return counties_with_damage\n",
    "\n",
    "    def sample_getter(self):\n",
    "        \"\"\" Randomly select 100 coastal and 100 inland MAINLAND counties. Clean county names so that cenusus county names match coastal county names,\n",
    "        that is make sure they end in COUNTY and are uppercase. Subset population data for those sample counties for the year of interest.\n",
    "        Write to csv.\"\"\"\n",
    "\n",
    "        # open relevant data files.\n",
    "        storm_df = pd.read_csv(self.data_paths[2], encoding='latin1', dtype=str)\n",
    "        census_df = pd.read_csv(self.data_paths[0], encoding='latin1', dtype=str)\n",
    "        coastal_counties_df = pd.read_csv(self.data_paths[1], encoding='latin1', dtype=str)\n",
    "\n",
    "\n",
    "        if self.ignore_zero_damage:\n",
    "            # remove counties with zero damage from storm data\n",
    "            storm_df = storm_df[storm_df['FIPS'].isin(self.counties_with_damage)]\n",
    "\n",
    "        ########################################################################\n",
    "        # randomly sample 100 coastal counties from list of coastal counties csv that are in mainland US and have non-zero damage if required.\n",
    "        ########################################################################\n",
    "        \n",
    "        # if ignoring zero damage, filter coastal counties to only those in counties_with_damage\n",
    "        if self.ignore_zero_damage:\n",
    "            coastal_counties_df = coastal_counties_df[coastal_counties_df['countyfips'].isin(self.counties_with_damage)]\n",
    "\n",
    "        # Randomly sample 100 coastal counties\n",
    "        coastal_counties_df_sample = coastal_counties_df.sample(n=100, random_state=self.random_seed)\n",
    "\n",
    "        # census: build full 5-digit FIPS code for matching\n",
    "        census_df['FIPS'] = census_df['STATE'] + census_df['COUNTY']   # 5-digit string\n",
    "\n",
    "        # Find the coastal counties that are also in census data by matching county fips.\n",
    "        self.coastal_sample = pd.merge(coastal_counties_df_sample, census_df, left_on=['countyfips'], right_on=['FIPS'], how='inner')\n",
    "        \n",
    "        # if less than 100 coastal counties matched, resample until we have 100\n",
    "        while len(self.coastal_sample) < 100:\n",
    "            #change seed for resampling\n",
    "            if self.random_seed is None:\n",
    "                self.random_seed = np.random.randint(1,1000)\n",
    "            self.random_seed += np.random.randint(1,1000)\n",
    "            np.random.seed(self.random_seed)\n",
    "            coastal_counties_df_sample = coastal_counties_df.sample(n=100, random_state=self.random_seed)\n",
    "            self.coastal_sample = pd.merge(coastal_counties_df_sample, census_df, left_on=['countyfips'], right_on=['FIPS'], how='inner')\n",
    "    \n",
    "        # Subset the census data to only include the columns of POPESTIMATE of the year, the county name, the state name, and the respective fips.\n",
    "        pop_col = f'POPESTIMATE{self.year}'\n",
    "        self.coastal_sample = self.coastal_sample[['countyfips','CTYNAME', 'STNAME', pop_col]]\n",
    "        \n",
    "        self.coastal_sample = self.coastal_sample.rename(columns={pop_col: 'Population'})\n",
    "        self.coastal_sample['Coastal'] = True\n",
    "\n",
    "        ########################################################################\n",
    "        # randomly select 100 inland counties from population data that are not in coastal counties and are mainland and have non-zero damage if required.\n",
    "        ########################################################################\n",
    "\n",
    "        # get list of coastal county fips to filter inland counties.\n",
    "        coastal_county_fips = coastal_counties_df['countyfips'].tolist()\n",
    "\n",
    "        # clean census data to not include all coastal counties. Also remove non-mainland states.\n",
    "        inland_counties_df = census_df[~census_df['FIPS'].isin(coastal_county_fips) & ~census_df['STATE'].isin(self.non_MAINLAND_STATES_FIPS)]\n",
    "\n",
    "        # if ignoring zero damage, filter inland counties to only those in counties_with_damage\n",
    "        if self.ignore_zero_damage:\n",
    "            inland_counties_df = inland_counties_df[inland_counties_df['FIPS'].isin(self.counties_with_damage)]\n",
    "        \n",
    "        # randomly sample 100 inland counties\n",
    "        self.inland_sample = inland_counties_df.sample(n=100, random_state=self.random_seed)\n",
    "\n",
    "        # subset population data to only those inland counties and for the year. \n",
    "        self.inland_sample = self.inland_sample[['FIPS','CTYNAME', 'STNAME', pop_col]]\n",
    "        #move fips column to match coastal sample\n",
    "        self.inland_sample = self.inland_sample.rename(columns={'FIPS':'countyfips'})\n",
    "        self.inland_sample = self.inland_sample.rename(columns={pop_col: 'Population'})\n",
    "        self.inland_sample['Coastal'] = False\n",
    "        \n",
    "        # concatenate coastal and inland samples\n",
    "        self.total_sample = pd.concat([self.coastal_sample, self.inland_sample], ignore_index=True)\n",
    "    \n",
    "    def zone_county_cleaner(self):\n",
    "        \"\"\" Add parent county fips to zone_county_reference dataframe for reference later.\"\"\"\n",
    "        zone_reference_df = pd.read_csv(self.zone_county_reference, dtype=str)\n",
    "\n",
    "        # Ensure all FIPS columns are clean, 5-digit strings, handling potential '.0'\n",
    "        zone_reference_df['countyfips'] = zone_reference_df['countyfips'].str.split('.').str[0].str.zfill(5)\n",
    "\n",
    "        #check if zone fips column already exists and is properly formatted\n",
    "        if 'zone_fips' in zone_reference_df.columns:\n",
    "            zone_reference_df['zone_fips'] = zone_reference_df['zone_fips'].str.split('.').str[0].str.zfill(5)\n",
    "            return zone_reference_df\n",
    "\n",
    "        # first find state fips from county fips by taking first two digits\n",
    "        zone_reference_df['state_fips'] = zone_reference_df['countyfips'].str[:2]\n",
    "\n",
    "        #now take last three digits from zone id and concat to state fips to make full zone fips\n",
    "        zone_reference_df['zone_fips'] = zone_reference_df['state_fips'] + zone_reference_df['Zone_ID'].str[-3:]\n",
    "        zone_reference_df['zone_fips'] = zone_reference_df['zone_fips'].str.zfill(5)\n",
    "\n",
    "\n",
    "        # overwrite zone_county_reference csv with new column\n",
    "        zone_reference_df.to_csv(self.zone_county_reference, index=False)\n",
    "\n",
    "        return zone_reference_df\n",
    "    \n",
    "    def map_county_to_zone(self):\n",
    "        \"\"\" Create a dictionary where the keys are zone fips and the values are all county fips in that zone.\"\"\"\n",
    "        # first run zone_county_cleaner to ensure zone fips are present\n",
    "        zone_reference_df = self.zone_county_cleaner()\n",
    "\n",
    "        # Create a mapping from county_fips to a list of zone_fips in that county.\n",
    "        county_to_zones = {}\n",
    "        for index, row in zone_reference_df.iterrows():\n",
    "            county_fips = row['countyfips']\n",
    "            zone_fips = row['zone_fips']\n",
    "            if zone_fips not in county_to_zones:\n",
    "                county_to_zones[zone_fips] = []\n",
    "            county_to_zones[zone_fips].append(county_fips)\n",
    "\n",
    "        return county_to_zones\n",
    "\n",
    "    def property_damage_getter(self):\n",
    "        \"\"\" For each county in self.coastal_sample and self.inland_sample, find average property damage per storm event for that year. \"\"\"\n",
    "\n",
    "        damage_dict = {}\n",
    "        storm_df = pd.read_csv(self.data_paths[2], encoding='latin1', dtype=str)\n",
    "\n",
    "\n",
    "        if self.zone:\n",
    "            # map county fips to zone fips\n",
    "            zone_ref_df = self.zone_county_cleaner()\n",
    "            county_to_zones_map = zone_ref_df.groupby('countyfips')['zone_fips'].apply(list).to_dict()\n",
    "        # else proceed as normal\n",
    "        else:\n",
    "            county_to_zones_map = {}\n",
    "\n",
    "        # if self.zero_damage is True, storm_df only has counties with non-zero damage already filtered in sample_getter\n",
    "\n",
    "        for index, row in self.total_sample.iterrows():\n",
    "            county_fips = row['countyfips']\n",
    "            \n",
    "            # Get all relevant FIPS codes: the county itself plus all its zones if needed.\n",
    "            fips_to_check = [county_fips]\n",
    "            if county_fips in county_to_zones_map:\n",
    "                fips_to_check.extend(county_to_zones_map[county_fips])\n",
    "            \n",
    "            # find the fips to check in storm data.\n",
    "            county_storms = storm_df[storm_df['FIPS'].isin(fips_to_check)]\n",
    "            \n",
    "            \n",
    "            # find the county in self.damage_by_county and get their total damage\n",
    "            if county_fips in self.damage_by_county.index:\n",
    "                total_property_damage = self.damage_by_county[county_fips]\n",
    "                total_storms = len(county_storms)\n",
    "                average_property_damage = total_property_damage / total_storms if total_storms > 0 else 0.0\n",
    "                damage_dict[county_fips] = average_property_damage\n",
    "            else:\n",
    "                damage_dict[county_fips] = 0.0\n",
    "\n",
    "        # add average property damage to self.total_sample\n",
    "        average_damages = []\n",
    "        for index, row in self.total_sample.iterrows():\n",
    "            county_fips = row['countyfips']\n",
    "            average_damages.append(damage_dict[county_fips])\n",
    "        self.total_sample[\"Property_Damage_per_Storm\"] = average_damages\n",
    "\n",
    "    def make_final_dataset(self):\n",
    "        \"\"\" Make data. Write final dataset for that year to the subfolder for that year. columns: CLeaned County Name, Cleaned State Name, Population, Average Property Damage, Coastal (T/F) \"\"\"\n",
    "        self.sample_getter()\n",
    "        self.property_damage_getter()\n",
    "\n",
    "        # determine final path based on zone and ignore_zero_damage\n",
    "        if self.zone and self.ignore_zero_damage:\n",
    "            final_path = self.data_path / f\"data_{self.year}_zone_{self.zone}_no_zero_damage\"\n",
    "        elif self.zone:\n",
    "            final_path = self.data_path / f\"data_{self.year}_with_zone\"\n",
    "        elif self.ignore_zero_damage:\n",
    "            final_path = self.data_path / f\"data_{self.year}_no_zero_damage\"\n",
    "        else:\n",
    "            final_path = self.data_path / f\"data_{self.year}\"\n",
    "        final_path = str(final_path) + f\"_random_seed_{self.random_seed}.csv\"\n",
    "\n",
    "        #write final dataset to csv\n",
    "        self.total_sample.to_csv(final_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataGetter(year=2024, ignore_zero_damage=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
